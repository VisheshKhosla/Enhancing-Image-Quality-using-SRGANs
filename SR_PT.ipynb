{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XZFrKWHwEf7h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\users\\dell\\anaconda3\\lib\\site-packages (0.11.2)\n",
      "Requirement already satisfied: torch==1.10.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from torchvision) (1.10.1)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from torchvision) (7.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\dell\\anaconda3\\lib\\site-packages (from torchvision) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dell\\anaconda3\\lib\\site-packages (from torch==1.10.1->torchvision) (3.7.4.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DsUYd7GhEf7m"
   },
   "source": [
    "Importing the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RC5YKYGuEf7n"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as n\n",
    "import torch.nn.functional as f\n",
    "import numpy as np\n",
    "import os\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torchvision import models\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2VN1wknpEf7q"
   },
   "source": [
    "Iterating through the number plate folder to get paths of all the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plate_data_path=\"generated\"\n",
    "state_wise_list=os.listdir(num_plate_data_path)\n",
    "images_path_list=[]\n",
    "for state in state_wise_list:\n",
    "    state_path=os.path.join(num_plate_data_path,state)\n",
    "    category_wise_list=os.listdir(state_path)\n",
    "    for category in category_wise_list:\n",
    "        category_path=os.path.join(state_path,category)\n",
    "        images_category_wise=os.listdir(category_path)\n",
    "        for image_num_plate in images_category_wise:\n",
    "            images_path_list.append(os.path.join(category_path,image_num_plate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4HEAwzAFEf7r"
   },
   "outputs": [],
   "source": [
    "# celebData = \"celeba-dataset/img_align_celeba/img_align_celeba/\"\n",
    "# images = os.listdir(celebData)\n",
    "# imageList = images[:1500]\n",
    "# imageList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dOAZSX60Ef7t",
    "outputId": "75856944-bc8f-4eea-d22c-1d3744657c11"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-9c07fc92abb3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'image_list' is not defined"
     ]
    }
   ],
   "source": [
    "# len(image_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5pvl-X2BEf7w"
   },
   "source": [
    "Assigning the device as cuda if GPU is available else cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3l4EmLQgEf7w"
   },
   "outputs": [],
   "source": [
    "cuda = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wtQvgarfEf7z"
   },
   "source": [
    "Generator Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yWoZ2TtvEf7z"
   },
   "outputs": [],
   "source": [
    "class Generator(n.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = n.Conv2d(3,64,9,padding=4,bias=False)\n",
    "        self.conv2 = n.Conv2d(64,64,3,padding=1,bias=False)\n",
    "        self.conv3_1 = n.Conv2d(64,256,3,padding=1,bias=False)\n",
    "        self.conv3_2 = n.Conv2d(64,256,3,padding=1,bias=False)\n",
    "        self.conv4 = n.Conv2d(64,3,9,padding=4,bias=False)\n",
    "        self.bn = n.BatchNorm2d(64)\n",
    "        self.ps = n.PixelShuffle(2)\n",
    "        self.prelu = n.PReLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        block1 = self.prelu(self.conv1(x))\n",
    "        block2 = torch.add(self.bn(self.conv2(self.prelu(self.bn(self.conv2(block1))))),block1)\n",
    "        block3 = torch.add(self.bn(self.conv2(self.prelu(self.bn(self.conv2(block2))))),block2)\n",
    "        block4 = torch.add(self.bn(self.conv2(self.prelu(self.bn(self.conv2(block3))))),block3)\n",
    "        block5 = torch.add(self.bn(self.conv2(self.prelu(self.bn(self.conv2(block4))))),block4)\n",
    "        block6 = torch.add(self.bn(self.conv2(self.prelu(self.bn(self.conv2(block5))))),block5)\n",
    "        block7 = torch.add(self.bn(self.conv2(block6)),block1)\n",
    "        block8 = self.prelu(self.ps(self.conv3_1(block7)))\n",
    "        block9 = self.prelu(self.ps(self.conv3_2(block8)))\n",
    "        block10 = self.conv4(block9)\n",
    "        return block10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yVcX-zDaEf71"
   },
   "source": [
    "Assigning the Generator to cuda(if available) and printing the summary of the Generator with a dummy input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kBHl0rO_Ef72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 64, 64]          15,552\n",
      "             PReLU-2           [-1, 64, 64, 64]               1\n",
      "            Conv2d-3           [-1, 64, 64, 64]          36,864\n",
      "       BatchNorm2d-4           [-1, 64, 64, 64]             128\n",
      "             PReLU-5           [-1, 64, 64, 64]               1\n",
      "            Conv2d-6           [-1, 64, 64, 64]          36,864\n",
      "       BatchNorm2d-7           [-1, 64, 64, 64]             128\n",
      "            Conv2d-8           [-1, 64, 64, 64]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 64, 64]             128\n",
      "            PReLU-10           [-1, 64, 64, 64]               1\n",
      "           Conv2d-11           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-12           [-1, 64, 64, 64]             128\n",
      "           Conv2d-13           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-14           [-1, 64, 64, 64]             128\n",
      "            PReLU-15           [-1, 64, 64, 64]               1\n",
      "           Conv2d-16           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-17           [-1, 64, 64, 64]             128\n",
      "           Conv2d-18           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-19           [-1, 64, 64, 64]             128\n",
      "            PReLU-20           [-1, 64, 64, 64]               1\n",
      "           Conv2d-21           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-22           [-1, 64, 64, 64]             128\n",
      "           Conv2d-23           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-24           [-1, 64, 64, 64]             128\n",
      "            PReLU-25           [-1, 64, 64, 64]               1\n",
      "           Conv2d-26           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-27           [-1, 64, 64, 64]             128\n",
      "           Conv2d-28           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-29           [-1, 64, 64, 64]             128\n",
      "           Conv2d-30          [-1, 256, 64, 64]         147,456\n",
      "     PixelShuffle-31         [-1, 64, 128, 128]               0\n",
      "            PReLU-32         [-1, 64, 128, 128]               1\n",
      "           Conv2d-33        [-1, 256, 128, 128]         147,456\n",
      "     PixelShuffle-34         [-1, 64, 256, 256]               0\n",
      "            PReLU-35         [-1, 64, 256, 256]               1\n",
      "           Conv2d-36          [-1, 3, 256, 256]          15,552\n",
      "================================================================\n",
      "Total params: 732,936\n",
      "Trainable params: 732,936\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 179.50\n",
      "Params size (MB): 2.80\n",
      "Estimated Total Size (MB): 182.34\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "gen = Generator().to(cuda)\n",
    "\n",
    "#Uncomment below mentioned three lines if you have more than one gpu and want to use all of them\n",
    "#ngpu=2\n",
    "# if (cuda.type == 'cuda') and (ngpu > 1):\n",
    "#     gen = n.DataParallel(gen, list(range(ngpu)))\n",
    "summary(gen,(3,64,64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nbQiATSjEf74"
   },
   "source": [
    "Discriminator Architecture along with a dropout probability of 0.3 to remove overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DAQ63HYsEf75"
   },
   "outputs": [],
   "source": [
    "class Discriminator(n.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = n.Conv2d(3,64,3,padding=1,bias=False)\n",
    "        self.conv2 = n.Conv2d(64,64,3,stride=2,padding=1,bias=False)\n",
    "        self.bn2 = n.BatchNorm2d(64)\n",
    "        self.conv3 = n.Conv2d(64,128,3,padding=1,bias=False)\n",
    "        self.bn3 = n.BatchNorm2d(128)\n",
    "        self.conv4 = n.Conv2d(128,128,3,stride=2,padding=1,bias=False)\n",
    "        self.bn4 = n.BatchNorm2d(128)\n",
    "        self.conv5 = n.Conv2d(128,256,3,padding=1,bias=False)\n",
    "        self.bn5 = n.BatchNorm2d(256)\n",
    "        self.conv6 = n.Conv2d(256,256,3,stride=2,padding=1,bias=False)\n",
    "        self.bn6 = n.BatchNorm2d(256)\n",
    "        self.conv7 = n.Conv2d(256,512,3,padding=1,bias=False)\n",
    "        self.bn7 = n.BatchNorm2d(512)\n",
    "        self.conv8 = n.Conv2d(512,512,3,stride=2,padding=1,bias=False)\n",
    "        self.bn8 = n.BatchNorm2d(512)\n",
    "        self.fc1 = n.Linear(512*16*16,1024)\n",
    "        self.fc2 = n.Linear(1024,1)\n",
    "        self.drop = n.Dropout2d(0.3)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        block1 = f.leaky_relu(self.conv1(x))\n",
    "        block2 = f.leaky_relu(self.bn2(self.conv2(block1)))\n",
    "        block3 = f.leaky_relu(self.bn3(self.conv3(block2)))\n",
    "        block4 = f.leaky_relu(self.bn4(self.conv4(block3)))\n",
    "        block5 = f.leaky_relu(self.bn5(self.conv5(block4)))\n",
    "        block6 = f.leaky_relu(self.bn6(self.conv6(block5)))\n",
    "        block7 = f.leaky_relu(self.bn7(self.conv7(block6)))\n",
    "        block8 = f.leaky_relu(self.bn8(self.conv8(block7)))\n",
    "        block8 = block8.view(-1,block8.size(1)*block8.size(2)*block8.size(3))\n",
    "        block9 = f.leaky_relu(self.fc1(block8),)\n",
    "        block10 = torch.sigmoid(self.drop(self.fc2(block9)))\n",
    "        return block9,block10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k2ub9ArZEf76"
   },
   "source": [
    "Assigning the discriminator to the gpu(if available) and printing the summary of the network with a dummy value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9GXi9cbgEf77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 256, 256]           1,728\n",
      "            Conv2d-2         [-1, 64, 128, 128]          36,864\n",
      "       BatchNorm2d-3         [-1, 64, 128, 128]             128\n",
      "            Conv2d-4        [-1, 128, 128, 128]          73,728\n",
      "       BatchNorm2d-5        [-1, 128, 128, 128]             256\n",
      "            Conv2d-6          [-1, 128, 64, 64]         147,456\n",
      "       BatchNorm2d-7          [-1, 128, 64, 64]             256\n",
      "            Conv2d-8          [-1, 256, 64, 64]         294,912\n",
      "       BatchNorm2d-9          [-1, 256, 64, 64]             512\n",
      "           Conv2d-10          [-1, 256, 32, 32]         589,824\n",
      "      BatchNorm2d-11          [-1, 256, 32, 32]             512\n",
      "           Conv2d-12          [-1, 512, 32, 32]       1,179,648\n",
      "      BatchNorm2d-13          [-1, 512, 32, 32]           1,024\n",
      "           Conv2d-14          [-1, 512, 16, 16]       2,359,296\n",
      "      BatchNorm2d-15          [-1, 512, 16, 16]           1,024\n",
      "           Linear-16                 [-1, 1024]     134,218,752\n",
      "           Linear-17                    [-1, 1]           1,025\n",
      "        Dropout2d-18                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 138,906,945\n",
      "Trainable params: 138,906,945\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 118.01\n",
      "Params size (MB): 529.89\n",
      "Estimated Total Size (MB): 648.65\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "disc = Discriminator().to(cuda)\n",
    "summary(disc,(3,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_2JfIT4bEf79"
   },
   "outputs": [],
   "source": [
    "disc = Discriminator().to(cuda).float()\n",
    "gen = Generator().to(cuda).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kVO4XWIFEf7_"
   },
   "source": [
    "Downloading the pretrained vgg19 model from model module of torchvision library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KCbJX4oyEf7_"
   },
   "outputs": [],
   "source": [
    "vgg = models.vgg19(pretrained=True).to(cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MLIIT_rvEf8B"
   },
   "source": [
    "Defining the losses to be used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rArq8bpwEf8B"
   },
   "outputs": [],
   "source": [
    "gen_loss = n.BCELoss()\n",
    "vgg_loss = n.MSELoss()\n",
    "mse_loss = n.MSELoss()\n",
    "disc_loss = n.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KpqcTlWKEf8D"
   },
   "source": [
    "Defining the adam optimizers for generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dNyZ946bEf8D"
   },
   "outputs": [],
   "source": [
    "gen_optimizer = optim.Adam(gen.parameters(),lr=0.0001)\n",
    "disc_optimizer = optim.Adam(disc.parameters(),lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pN7aKN62Ef8G"
   },
   "source": [
    "Loading the images after based on resizing as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GR_RwD4zEf8G"
   },
   "outputs": [],
   "source": [
    "def loadImages(images_path_list,resize=False):\n",
    "    images=[]\n",
    "    for image_path in (images_path_list):\n",
    "        if resize:\n",
    "            img = cv2.resize(cv2.imread(image_path),(256,256)) \n",
    "        else:\n",
    "            img = cv2.imread(image_path)\n",
    "        img = np.moveaxis(img, 2, 0)\n",
    "        images.append(img)\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8PnAJcu1Ef8I"
   },
   "source": [
    "Converting the high resolution images into low resolution by applying gaussian blur, resizing in to 64*64 and loading it as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-nv01VaFEf8I"
   },
   "outputs": [],
   "source": [
    "def loadLRImages(images_path_list):\n",
    "    images=[]\n",
    "    for image_path in (images_path_list):\n",
    "        img = cv2.resize(cv2.GaussianBlur(cv2.imread(image_path),(5,5),cv2.BORDER_DEFAULT),(64,64)) \n",
    "        img = np.moveaxis(img, 2, 0)\n",
    "        images.append(img)\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xhpVOaMvEf8L"
   },
   "source": [
    "Loading the generator model from the given checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uoI7nAl8Ef8L"
   },
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model = checkpoint['model']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    for parameter in model.parameters():\n",
    "        parameter.requires_grad = False\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fGQDWlS0Ef8N"
   },
   "source": [
    "Given low resolution images and checkpoint, Generating the high resolution images out of it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_MU7hpP6Ef8N"
   },
   "outputs": [],
   "source": [
    "def imagePostProcess(imagedir,modelPath):\n",
    "    imagelist=[]\n",
    "#     images = os.listdir(imagedir)\n",
    "    for img in imagedir:\n",
    "        img = cv2.resize(cv2.GaussianBlur(cv2.imread(os.path.join(hr_path,img)),(5,5),cv2.BORDER_DEFAULT),(64,64)) \n",
    "        imagelist.append(img)\n",
    "    imagearray = np.array(imagelist)/255\n",
    "#     imagearray = (imagedir)/255\n",
    "    # imagearrayPT = np.reshape(imagearray,(len(imagelist),imagearray.shape[3],imagearray.shape[1],imagearray.shape[2]))\n",
    "    imagearrayPT = np.moveaxis(imagearray,3,1)\n",
    "    # print(imagearrayPT.shape)\n",
    "\n",
    "    model = load_checkpoint(modelPath)\n",
    "    im_tensor = torch.from_numpy(imagearrayPT).float()\n",
    "    out_tensor = model(im_tensor)\n",
    "    # print(out_tensor.shape)\n",
    "    # out = np.reshape(out_tensor,[out_tensor.shape[0],out_tensor.shape[2],out_tensor.shape[3],out_tensor.shape[1]])\n",
    "    out = out_tensor.numpy()\n",
    "    out = np.moveaxis(out,1,3)\n",
    "    # print(out.shape)\n",
    "    out = np.clip(out,0,1)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LJ_ukw9GEf8P"
   },
   "source": [
    "Display utility of displaying images using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aB-SFmTlEf8P"
   },
   "outputs": [],
   "source": [
    "def show_samples(sample_images):\n",
    "    figure, axes = plt.subplots(1, sample_images.shape[0], figsize = (10,10))\n",
    "    for index, axis in enumerate(axes):\n",
    "        axis.axis('off')\n",
    "        image_array = sample_images[index]\n",
    "        axis.imshow(image_array)\n",
    "        image = Image.fromarray((image_array * 255).astype('uint8'))\n",
    "    plt.savefig(os.path.join(base_path,\"out/SR\")+\"_\"+str(epoch)+\".png\", bbox_inches='tight', pad_inches=0)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xZBTT9lBEf8R"
   },
   "outputs": [],
   "source": [
    "#change the batch-size based on your system memory\n",
    "\n",
    "epochs=1000\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fzY7B-GCEf8T",
    "outputId": "50e0f9ae-7fce-4333-ac87-7fa8072c68cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "base_path = os.getcwd()\n",
    "\n",
    "#lr_path = os.path.join(base_path,\"trainImages\")\n",
    "# hr_path =celebData\n",
    "#valid_path = os.path.join(base_path,\"SR_valid\")\n",
    "weight_file = os.path.join(base_path,\"SRPT_weights\")\n",
    "out_path = os.path.join(base_path,\"out\")\n",
    "\n",
    "if not os.path.exists(weight_file):\n",
    "    os.makedirs(weight_file)\n",
    "\n",
    "if not os.path.exists(out_path):\n",
    "    os.makedirs(out_path)\n",
    "\n",
    "    \n",
    "#LR_images_list = os.listdir(lr_path)\n",
    "HR_images_list = images_path_list\n",
    "batch_count = len(HR_images_list)//batch_size\n",
    "batch_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1pwc5pMhEf8U"
   },
   "source": [
    "Starting of training and defining losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "colab_type": "code",
    "id": "L1p5wFAzEf8V",
    "outputId": "68224ed4-b26a-4d86-abad-881f7ab1e25f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                           | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:76] data. DefaultCPUAllocator: not enough memory: you tried to allocate 1572864000 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-4df67c3acef8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mdisc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mgen_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdisc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdisc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhr_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-6299a54b3ace>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mblock1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mblock2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mblock1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mblock3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mblock2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mblock4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mblock3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mblock5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mblock4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    442\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[1;32m--> 443\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:76] data. DefaultCPUAllocator: not enough memory: you tried to allocate 1572864000 bytes."
     ]
    }
   ],
   "source": [
    "#batch_count=60\n",
    "for epoch in range(epochs):\n",
    "    df_loss_list=[]\n",
    "    dr_loss_list=[]\n",
    "    gloss_list=[]\n",
    "    vloss_list=[]\n",
    "    mloss_list=[]\n",
    "    \n",
    "    for batch in tqdm(range(batch_count)):\n",
    "        hr_imagesList = [img for img in HR_images_list[batch*batch_size:(batch+1)*batch_size]]\n",
    "        lr_images = loadLRImages(images_path_list)/255\n",
    "        hr_images = loadImages(images_path_list,True)/255\n",
    "        \n",
    "        disc.zero_grad()\n",
    "\n",
    "        gen_out = gen(torch.from_numpy(lr_images).to(cuda).float())\n",
    "        _,f_label = disc(gen_out)\n",
    "        _,r_label = disc(torch.from_numpy(hr_images).to(cuda).float())\n",
    "        df_loss = (disc_loss(f_label,torch.zeros_like(f_label,dtype=torch.float)))\n",
    "        dr_loss = (disc_loss(r_label,torch.ones_like(r_label,dtype=torch.float)))\n",
    "        # discriminator bce loss = df_loss + dr_loss\n",
    "        df_loss.backward()\n",
    "        dr_loss.backward(retain_graph=True)\n",
    "        disc_optimizer.step()\n",
    "        \n",
    "        gen.zero_grad()      \n",
    "        g_loss = gen_loss(f_label.data,torch.ones_like(f_label,dtype=torch.float))\n",
    "        v_loss = vgg_loss(vgg.features[:7](gen_out),vgg.features[:7](torch.from_numpy(hr_images).to(cuda).float()))\n",
    "        m_loss = mse_loss(gen_out,torch.from_numpy(hr_images).to(cuda).float())\n",
    "        \n",
    "        generator_loss = g_loss + v_loss + m_loss\n",
    "        \n",
    "\n",
    "        generator_loss.backward()\n",
    "        gen_optimizer.step()\n",
    "        \n",
    "        df_loss_list.append(df_loss.item())\n",
    "        dr_loss_list.append(dr_loss.item())\n",
    "        \n",
    "        gloss_list.append(g_loss.item())\n",
    "        vloss_list.append(v_loss.item())\n",
    "        mloss_list.append(m_loss.item())\n",
    "\n",
    "        \n",
    "        \n",
    "        print(\"d1Loss ::: \"+str((d1_loss.item()))+\" d2Loss ::: \"+str((d2_loss.item())))\n",
    "        print(\"gloss ::: \"+str((g_loss.item()))+\" vloss ::: \"+str((v_loss.item()))+\" mloss ::: \"+str((m_loss.item())))\n",
    "    print(\"Epoch ::::  \"+str(epoch+1)+\"  d1_loss ::: \"+str(np.mean(d1loss_list))+\"  d2_loss :::\"+str(np.mean(d2loss_list)))\n",
    "    print(\"genLoss ::: \"+str(np.mean(gloss_list))+\"  vggLoss ::: \"+str(np.mean(vloss_list))+\"  MeanLoss  ::: \"+str(np.mean(mloss_list)))\n",
    "    \n",
    "    if(epoch%3==0):\n",
    "        \n",
    "        checkpoint = {'model': Generator(),\n",
    "              'input_size': 64,\n",
    "              'output_size': 256,\n",
    "              'state_dict': gen.state_dict()}\n",
    "        torch.save(checkpoint,os.path.join(weight_file,\"SR\"+str(epoch+1)+\".pth\"))\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        out_images = imagePostProcess(images[-2:],os.path.join(weight_file,\"SR\"+str(epoch+1)+\".pth\"))\n",
    "#         print(out_images.shape)\n",
    "#         test_images = loadLRImages(images[:-3],hr_path)/255\n",
    "#         test_images = np.reshape(test_images,(test_images[0],test_images.shape[3],test_images.shape[1],test_images.shape[2]))\n",
    "#         out_images = gen(torch.from_numpy(test_images).to(cuda).float())\n",
    "#         out_images = np.reshape(out_images,(out_images[0],out_images[2],out_images[3],out_images[1]))\n",
    "        show_samples(out_images)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "SR_PT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
